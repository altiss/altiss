{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iwe_m2d_win_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/altiss/altiss/blob/main/iwe_m2d_win_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SapowXFcfr3"
      },
      "source": [
        "#VIC put here all the new imports that may be needed\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import h5py\n",
        "import sklearn.metrics\n",
        "import torch.nn as nn\n",
        "from scipy.ndimage import gaussian_filter\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# from utilities3 import *\n",
        "\n",
        "import operator\n",
        "from functools import reduce\n",
        "from functools import partial\n",
        "\n",
        "from timeit import default_timer\n",
        "\n",
        "import os, os.path\n",
        "\n",
        "import torch\n",
        "\n",
        "v = torch.__version__\n",
        "# assert(v[2] == '8')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHKPa0CaS47c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d911b7f-9aeb-4039-9907-d99057997f93"
      },
      "source": [
        "\n",
        "# mount my google drive to access dataset and save model\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "neural_solver\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnAf4NpSdQoA"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg_DT5rrdSUe"
      },
      "source": [
        "#VIC this is the content of: https://github.com/zongyi-li/fourier_neural_operator/blob/master/utilities3.py\n",
        "# it may need to be udpated\n",
        "\n",
        "#################################################\n",
        "#\n",
        "# Utilities\n",
        "#\n",
        "#################################################\n",
        "# reading data\n",
        "class MatReader(object):\n",
        "    def __init__(self, file_path, to_torch=True, to_cuda=False, to_float=True):\n",
        "        super(MatReader, self).__init__()\n",
        "\n",
        "        self.to_torch = to_torch\n",
        "        self.to_cuda = to_cuda\n",
        "        self.to_float = to_float\n",
        "\n",
        "        self.file_path = file_path\n",
        "\n",
        "        self.data = None\n",
        "        self.old_mat = None\n",
        "        self._load_file()\n",
        "\n",
        "    def _load_file(self):\n",
        "        try:\n",
        "            self.data = scipy.io.loadmat(self.file_path)\n",
        "            self.old_mat = True\n",
        "        except:\n",
        "            self.data = h5py.File(self.file_path)\n",
        "            self.old_mat = False\n",
        "\n",
        "    def load_file(self, file_path):\n",
        "        self.file_path = file_path\n",
        "        self._load_file()\n",
        "\n",
        "    def read_field(self, field):\n",
        "        x = self.data[field]\n",
        "\n",
        "        if not self.old_mat:\n",
        "            x = x[()]\n",
        "            x = np.transpose(x, axes=range(len(x.shape) - 1, -1, -1))\n",
        "\n",
        "        if self.to_float:\n",
        "            x = x.astype(np.float32)\n",
        "\n",
        "        if self.to_torch:\n",
        "            x = torch.from_numpy(x)\n",
        "\n",
        "            if self.to_cuda:\n",
        "                x = x.cuda()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def set_cuda(self, to_cuda):\n",
        "        self.to_cuda = to_cuda\n",
        "\n",
        "    def set_torch(self, to_torch):\n",
        "        self.to_torch = to_torch\n",
        "\n",
        "    def set_float(self, to_float):\n",
        "        self.to_float = to_float\n",
        "\n",
        "# normalization, pointwise gaussian\n",
        "class UnitGaussianNormalizer(object):\n",
        "    def __init__(self, x, eps=0.00001):\n",
        "        super(UnitGaussianNormalizer, self).__init__()\n",
        "\n",
        "        # x could be in shape of ntrain*n or ntrain*T*n or ntrain*n*T\n",
        "        self.mean = torch.mean(x, 0)\n",
        "        self.std = torch.std(x, 0)\n",
        "        self.eps = eps\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = (x - self.mean) / (self.std + self.eps)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        if sample_idx is None:\n",
        "            std = self.std + self.eps # n\n",
        "            mean = self.mean\n",
        "        else:\n",
        "            if len(self.mean.shape) == len(sample_idx[0].shape):\n",
        "                std = self.std[sample_idx] + self.eps  # batch*n\n",
        "                mean = self.mean[sample_idx]\n",
        "            if len(self.mean.shape) > len(sample_idx[0].shape):\n",
        "                std = self.std[:,sample_idx]+ self.eps # T*batch*n\n",
        "                mean = self.mean[:,sample_idx]\n",
        "\n",
        "        # x is in shape of batch*n or T*batch*n\n",
        "        x = (x * std) + mean\n",
        "        return x\n",
        "\n",
        "    def cuda(self):\n",
        "        self.mean = self.mean.cuda()\n",
        "        self.std = self.std.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        self.mean = self.mean.cpu()\n",
        "        self.std = self.std.cpu()\n",
        "\n",
        "# normalization, Gaussian\n",
        "class GaussianNormalizer(object):\n",
        "    def __init__(self, x, eps=0.00001):\n",
        "        super(GaussianNormalizer, self).__init__()\n",
        "\n",
        "        self.mean = torch.mean(x)\n",
        "        self.std = torch.std(x)\n",
        "        self.eps = eps\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = (x - self.mean) / (self.std + self.eps)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, sample_idx=None):\n",
        "        x = (x * (self.std + self.eps)) + self.mean\n",
        "        return x\n",
        "\n",
        "    def cuda(self):\n",
        "        self.mean = self.mean.cuda()\n",
        "        self.std = self.std.cuda()\n",
        "\n",
        "    def cpu(self):\n",
        "        self.mean = self.mean.cpu()\n",
        "        self.std = self.std.cpu()\n",
        "\n",
        "\n",
        "# normalization, scaling by range\n",
        "class RangeNormalizer(object):\n",
        "    def __init__(self, x, low=0.0, high=1.0):\n",
        "        super(RangeNormalizer, self).__init__()\n",
        "        mymin = torch.min(x, 0)[0].view(-1)\n",
        "        mymax = torch.max(x, 0)[0].view(-1)\n",
        "\n",
        "        self.a = (high - low)/(mymax - mymin)\n",
        "        self.b = -self.a*mymax + high\n",
        "\n",
        "    def encode(self, x):\n",
        "        s = x.size()\n",
        "        x = x.view(s[0], -1)\n",
        "        x = self.a*x + self.b\n",
        "        x = x.view(s)\n",
        "        return x\n",
        "\n",
        "    def decode(self, x):\n",
        "        s = x.size()\n",
        "        x = x.view(s[0], -1)\n",
        "        x = (x - self.b)/self.a\n",
        "        x = x.view(s)\n",
        "        return x\n",
        "\n",
        "#loss function with rel/abs Lp loss\n",
        "class LpLoss(object):\n",
        "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
        "        super(LpLoss, self).__init__()\n",
        "\n",
        "        #Dimension and Lp-norm type are postive\n",
        "        assert d > 0 and p > 0\n",
        "\n",
        "        self.d = d\n",
        "        self.p = p\n",
        "        self.reduction = reduction\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def abs(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "\n",
        "        #Assume uniform mesh\n",
        "        h = 1.0 / (x.size()[1] - 1.0)\n",
        "\n",
        "        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)\n",
        "\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(all_norms)\n",
        "            else:\n",
        "                return torch.sum(all_norms)\n",
        "\n",
        "        return all_norms\n",
        "\n",
        "    def rel(self, x, y):\n",
        "        num_examples = x.size()[0]\n",
        "\n",
        "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
        "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
        "\n",
        "        if self.reduction:\n",
        "            if self.size_average:\n",
        "                return torch.mean(diff_norms/y_norms)\n",
        "            else:\n",
        "                return torch.sum(diff_norms/y_norms)\n",
        "\n",
        "        return diff_norms/y_norms\n",
        "\n",
        "    def __call__(self, x, y):\n",
        "        return self.rel(x, y)\n",
        "\n",
        "# A simple feedforward neural network\n",
        "class DenseNet(torch.nn.Module):\n",
        "    def __init__(self, layers, nonlinearity, out_nonlinearity=None, normalize=False):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        self.n_layers = len(layers) - 1\n",
        "\n",
        "        assert self.n_layers >= 1\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for j in range(self.n_layers):\n",
        "            self.layers.append(nn.Linear(layers[j], layers[j+1]))\n",
        "\n",
        "            if j != self.n_layers - 1:\n",
        "                if normalize:\n",
        "                    self.layers.append(nn.BatchNorm1d(layers[j+1]))\n",
        "\n",
        "                self.layers.append(nonlinearity())\n",
        "\n",
        "        if out_nonlinearity is not None:\n",
        "            self.layers.append(out_nonlinearity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        for _, l in enumerate(self.layers):\n",
        "            x = l(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLgIy7iedqsR"
      },
      "source": [
        "# Model builder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzkTeyuZdu6V"
      },
      "source": [
        "#VIC this is the content of: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_2d_time.py\n",
        "# it needs to be udpated!\n",
        "# i made a small modification to the original code, please try to preserve it when updating it\n",
        "# the mod is highlighted by the following text #VIC-mod\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "################################################################\n",
        "# fourier layer\n",
        "################################################################\n",
        "\n",
        "class SpectralConv2d_fast(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
        "        super(SpectralConv2d_fast, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
        "        \"\"\"\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
        "        self.modes2 = modes2\n",
        "\n",
        "        self.scale = (1 / (in_channels * out_channels))\n",
        "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
        "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
        "\n",
        "    # Complex multiplication\n",
        "    def compl_mul2d(self, input, weights):\n",
        "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
        "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
        "        x_ft = torch.fft.rfft2(x)\n",
        "\n",
        "        # Multiply relevant Fourier modes\n",
        "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
        "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
        "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
        "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
        "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
        "\n",
        "        #Return to physical space\n",
        "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
        "        return x\n",
        "\n",
        "class SimpleBlock2d(nn.Module):\n",
        "    def __init__(self, modes1, modes2, width, t_in):\n",
        "        super(SimpleBlock2d, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        The overall network. It contains 4 layers of the Fourier layer.\n",
        "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
        "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
        "            W defined by self.w; K defined by self.conv .\n",
        "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
        "        \n",
        "        input: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
        "        input shape: (batchsize, x=64, y=64, c=12)\n",
        "        output: the solution of the next timestep\n",
        "        output shape: (batchsize, x=64, y=64, c=1)\n",
        "        \"\"\"\n",
        "\n",
        "        self.modes1 = modes1\n",
        "        self.modes2 = modes2\n",
        "        self.width = width\n",
        "        #self.fc0 = nn.Linear(12, self.width)\n",
        "        # input channel is 12: the solution of the previous 10 timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
        "        \n",
        "        #VIC-mod t_in is passed as parameter now, so that we can decide the number of input time steps\n",
        "        self.fc0 = nn.Linear(t_in+2, self.width)\n",
        "        # input channel: the solution of the previous t_in timesteps + 2 locations (u(t-10, x, y), ..., u(t-1, x, y),  x, y)\n",
        "\n",
        "\n",
        "        self.conv0 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
        "        self.conv1 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
        "        self.conv2 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
        "        self.conv3 = SpectralConv2d_fast(self.width, self.width, self.modes1, self.modes2)\n",
        "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.w3 = nn.Conv1d(self.width, self.width, 1)\n",
        "        self.bn0 = torch.nn.BatchNorm2d(self.width)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(self.width)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(self.width)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(self.width)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.width, 128)\n",
        "        self.fc2 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batchsize = x.shape[0]\n",
        "        size_x, size_y = x.shape[1], x.shape[2]\n",
        "\n",
        "        grid = self.get_grid(batchsize, size_x, size_y, x.device)\n",
        "        x = torch.cat((x, grid), dim=-1)\n",
        "        x = self.fc0(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "\n",
        "        x1 = self.conv0(x)\n",
        "        x2 = self.w0(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
        "        x = self.bn0(x1 + x2)\n",
        "        x = F.relu(x)\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.w1(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
        "        x = self.bn1(x1 + x2)\n",
        "        x = F.relu(x)\n",
        "        x1 = self.conv2(x)\n",
        "        x2 = self.w2(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
        "        x = self.bn2(x1 + x2)\n",
        "        x = F.relu(x)\n",
        "        x1 = self.conv3(x)\n",
        "        x2 = self.w3(x.view(batchsize, self.width, -1)).view(batchsize, self.width, size_x, size_y)\n",
        "        x = self.bn3(x1 + x2)\n",
        "\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def get_grid(self, batchsize, size_x, size_y, device):\n",
        "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
        "        gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
        "        gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
        "        gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
        "        return torch.cat((gridx, gridy), dim=-1).to(device)\n",
        "\n",
        "class Net2d(nn.Module):\n",
        "    def __init__(self, modes, width, t_in):\n",
        "        super(Net2d, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        A wrapper function\n",
        "        \"\"\"\n",
        "\n",
        "        self.conv1 = SimpleBlock2d(modes, modes, width, t_in)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def count_params(self):\n",
        "        c = 0\n",
        "        for p in self.parameters():\n",
        "            c += reduce(operator.mul, list(p.size()))\n",
        "\n",
        "        return c"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWQMCjxIgp0K"
      },
      "source": [
        "# Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-E0G_JAgsiz"
      },
      "source": [
        "#VIC leave this as it is\n",
        "\n",
        "def dataset_loader(dataset_name, dataset_path, n, win, stride=1, win_lim=-1) :\n",
        "  # get N, T, w and h from file name\n",
        "  datadetails = dataset_name.split(\"_\")\n",
        "  N = datadetails[2][1:]\n",
        "  N = int(N) # num of dataset entries\n",
        "  T = datadetails[3][1:]\n",
        "  T = int(T) # timesteps of each dataset entry\n",
        "  w = datadetails[4][1:]\n",
        "  w = int(w)\n",
        "  h = int(w)\n",
        "  # all the other parameters are dataset specific!\n",
        "\n",
        "  # to grab only a subset of the timesteps in each data entry\n",
        "  if win_lim != -1 and win_lim < T:\n",
        "    T = win_lim\n",
        "\n",
        "  # check that window size is smaller than number of timesteps per each data entry\n",
        "  assert (T >= win)  \n",
        "\n",
        "  # each entry in the dataset is now split in several trainig points, as big as T_in+T_out\n",
        "  #p_num = T-(win-1) # number of points per each dataset entry  \n",
        "  p_num = int( (T-win)/stride ) +1 # number of points per each dataset entry  \n",
        "  p_tot = N * p_num # all training points in dataset\n",
        "  print('Availble points in dataset: ', p_tot)\n",
        "  print('Points requested: ', n)\n",
        "  assert (p_tot >= n)  \n",
        "\n",
        "  # count number of checkpoints, their size and check for remainder file\n",
        "  dataset_full_path = dataset_path + dataset_name + '/'\n",
        "\n",
        "  files = os.listdir(dataset_full_path)\n",
        "  cp = len(files)\n",
        "  rem = 0\n",
        "\n",
        "  for name in files :\n",
        "    splitname = name.split(\"_\")\n",
        "    if splitname[-2] == 'rem' :\n",
        "      rem = 1\n",
        "      cp = cp-1\n",
        "      break\n",
        "\n",
        "  files = sorted(files) # order checkpoint files\n",
        "  cp_size = files[0].split(\"_\")[-1].split(\".\")[0] # read number of dataset entries in each checkpoint\n",
        "  cp_size = int(cp_size)\n",
        "  #cp_size = N//cp # number of dataset entries in each checkpoint\n",
        "  rem_size = 0 # number of dataset entries in remainder file [if any]\n",
        "  if rem > 0 :\n",
        "    rem_size = N - (cp*cp_size)\n",
        "\n",
        "  #print(N, T, w, h, cp, cp_size, rem, rem_size)\n",
        "\n",
        "  # prepare tensor where to load requested data points\n",
        "  u = torch.zeros(n, h, w, win)\n",
        "  #print(u.shape)\n",
        "\n",
        "  # actual sizes with moving window\n",
        "\n",
        "  cp_size_p = cp_size * p_num # number of points per each check point\n",
        "  rem_size_p = rem_size * p_num # number of points in remainder\n",
        "\n",
        "  # let's load\n",
        "\n",
        "  # check how many files we need to cover n points\n",
        "  full_files = n//(cp_size_p)\n",
        "  extra_datapoints = n%cp_size_p\n",
        "\n",
        "  extra_file_needed = extra_datapoints>0\n",
        "\n",
        "  print('Retrieved over', full_files, 'full files,', cp_size_p, 'points each')\n",
        "\n",
        "  # check that all numbers are fine\n",
        "  assert (full_files+extra_file_needed <= cp+rem)\n",
        "\n",
        "  \n",
        "  #print(files)\n",
        "  \n",
        "\n",
        "  # first load from files we will read completely \n",
        "  cnt = 0\n",
        "  for f in range(0,full_files) :\n",
        "    dataloader = MatReader(dataset_full_path+files[f])\n",
        "    uu = dataloader.read_field('u')\n",
        "    #print(f, files[f])\n",
        "    # unroll all entries with moving window\n",
        "    for e in range(0, cp_size) :\n",
        "      # window extracts p_num points from each dataset entry\n",
        "      #print('e', e, 'p_num', p_num)\n",
        "      for tt in range(0, p_num) :\n",
        "        #print('tt', tt)\n",
        "        t = tt*stride\n",
        "        #print('t', t)\n",
        "        u[cnt:cnt+1,...] = uu[e,:,:,t:t+win]\n",
        "        cnt = cnt+1\n",
        "\n",
        "  #print(cnt, extra_datapoints)\n",
        "  \n",
        "\n",
        "  # then load any possible remainder from a further file\n",
        "  if extra_datapoints>0 :\n",
        "    print('Plus', extra_datapoints, 'points from further file')\n",
        "    extra_entries = (extra_datapoints+0.5)//p_num # ceiling to be sure to have enough entries to unroll\n",
        "    dataloader = MatReader(dataset_full_path+files[full_files])\n",
        "    uu = dataloader.read_field('u')\n",
        "    entry = -1\n",
        "    while cnt < n :\n",
        "      entry = entry+1\n",
        "      for tt in range(0,p_num) :\n",
        "        t = tt*stride\n",
        "        u[cnt:cnt+1,...] = uu[entry,:,:,t:t+win] \n",
        "        cnt = cnt+1\n",
        "        if cnt >= n :\n",
        "          break\n",
        "\n",
        "  return u"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeHE_R1vd9de"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dNGG72ZeABN",
        "outputId": "2a1dc0d3-8b63-444c-952a-a86b89891eba"
      },
      "source": [
        "#VIC leave this as it is\n",
        "# then you will be able to play around with the settings\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# editable simulation parameters\n",
        "ntrain = 15000\n",
        "ntest = 2000\n",
        "\n",
        "modes = 12\n",
        "width = 32\n",
        "\n",
        "batch_size = 20\n",
        "\n",
        "epochs = 500\n",
        "learning_rate = 0.0025\n",
        "scheduler_step = 100\n",
        "scheduler_gamma = 0.5\n",
        "\n",
        "print(epochs, learning_rate, scheduler_step, scheduler_gamma)\n",
        "\n",
        "T_in = 10\n",
        "T_out = 10\n",
        "# T_in+T_out is window size!\n",
        "\n",
        "win_stride = 1\n",
        "win_lim = -1 #(T_in+T_out)*200 #-1 for no limit\n",
        "\n",
        "# dataset\n",
        "dataset_name = 'iwe_d0_n1000_t50_s64_mu0@1_rho0@5_gamma1'\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "MODEL_ID = '2d_win'\n",
        "\n",
        "dataset_path = '/content/drive/My Drive/Colab Notebooks/neural_solver/wave_equation/irreducible/datasets/'\n",
        "\n",
        "# retrieve dataset details and check them\n",
        "splitname = dataset_name.split('_')\n",
        "\n",
        "DATASET = splitname[1]\n",
        "\n",
        "S = splitname[4]\n",
        "S = int(S[1:])\n",
        "\n",
        "mu = splitname[5][2:]\n",
        "rho = splitname[6][3:]\n",
        "gamma = splitname[7][5:]\n",
        "\n",
        "# prepare to save model\n",
        "model_name = 'iwe_m'+MODEL_ID+'_'+DATASET+'_n'+str(ntrain)+'+'+str(ntest)+'_e'+str(epochs)+'_m'+str(modes)+'_w'+ str(width)+'_ti'+str(T_in)+'_to'+str(T_out)+'_ws'+str(win_stride)+'_wl'+str(win_lim)+'_s'+str(S)+'_m'+mu+'_r'+rho+'_g'+gamma\n",
        "\n",
        "model_path = dataset_path[:-9]+'/models/'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 0.0025 100 0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPUr0mA2d3TA"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8htbNF3ro6Ky",
        "outputId": "5a066bd4-5239-4aeb-805c-ea46bbc5ab7b"
      },
      "source": [
        "#VIC i think this needs only a minor update, i.e., removing the padding of the location\n",
        "\n",
        "t1 = default_timer()\n",
        "\n",
        "u = dataset_loader(dataset_name, dataset_path, ntrain+ntest, T_in+T_out, win_stride, win_lim)\n",
        "\n",
        "train_a = u[:ntrain,:,:,:T_in]\n",
        "train_u = u[:ntrain,:,:,T_in:T_in+T_out]\n",
        "\n",
        "ntest_start = ntrain\n",
        "test_a = u[ntest_start:ntest_start+ntest,:,:,:T_in]\n",
        "test_u = u[ntest_start:ntest_start+ntest:,:,:,T_in:T_in+T_out]\n",
        "\n",
        "#test_a = u[-ntest:,:,:,:T_in]\n",
        "#test_u = u[-ntest:,:,:,T_in:T_in+T_out]\n",
        "\n",
        "\n",
        "print(train_u.shape, test_u.shape)\n",
        "assert (S == train_u.shape[-2])\n",
        "assert (T_out == train_u.shape[-1])\n",
        "\n",
        "train_a = train_a.reshape(ntrain,S,S,T_in)\n",
        "test_a = test_a.reshape(ntest,S,S,T_in)\n",
        "\n",
        "#VIC should be removed\n",
        "# pad the location (x,y)\n",
        "# gridx = torch.tensor(np.linspace(0, 1, S), dtype=torch.float)\n",
        "# gridx = gridx.reshape(1, S, 1, 1).repeat([1, 1, S, 1])\n",
        "# gridy = torch.tensor(np.linspace(0, 1, S), dtype=torch.float)\n",
        "# gridy = gridy.reshape(1, 1, S, 1).repeat([1, S, 1, 1])\n",
        "\n",
        "# train_a = torch.cat((gridx.repeat([ntrain,1,1,1]), gridy.repeat([ntrain,1,1,1]), train_a), dim=-1)\n",
        "# test_a = torch.cat((gridx.repeat([ntest,1,1,1]), gridy.repeat([ntest,1,1,1]), test_a), dim=-1)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a, train_u), batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_a, test_u), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "t2 = default_timer()\n",
        "\n",
        "print('preprocessing finished, time used:', t2-t1, 's')\n",
        "print('train input shape:',train_a.shape, ' output shape: ', train_u.shape)    "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Availble points in dataset:  31000\n",
            "Points requested:  17000\n",
            "Retrieved over 3 full files, 4960 points each\n",
            "Plus 2120 points from further file\n",
            "torch.Size([15000, 64, 64, 10]) torch.Size([2000, 64, 64, 10])\n",
            "preprocessing finished, time used: 20.848891074000008 s\n",
            "train input shape: torch.Size([15000, 64, 64, 10])  output shape:  torch.Size([15000, 64, 64, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y46kBR84qIsa"
      },
      "source": [
        "# Build and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x6rURfQqLCf",
        "outputId": "61dd9987-b7e4-4f5f-dc24-96eac09c21c9"
      },
      "source": [
        "#VIC this needs a couple of touch ups, as at the bottom of: https://github.com/zongyi-li/fourier_neural_operator/blob/master/fourier_2d_time.py\n",
        "# also, i made very minor changes to the original code, to make the logic clearer\n",
        "\n",
        "if torch.cuda.is_available() :\n",
        "  model = Net2d(modes, width, T_in).cuda()\n",
        "  device  = torch.device('cuda')\n",
        "else :\n",
        "  model = Net2d(modes, width, T_in)\n",
        "  device  = torch.device('cpu')\n",
        "\n",
        "\n",
        "print(model.count_params())\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
        "\n",
        "\n",
        "# myloss = LpLoss(size_average=False)\n",
        "\n",
        "#VIC these are not needed anymore\n",
        "# gridx = gridx.to(device)\n",
        "# gridy = gridy.to(device)\n",
        "\n",
        "\n",
        "\n",
        "myloss = LpLoss(size_average=False)\n",
        "step = 1\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    t1 = default_timer()\n",
        "    train_l2_step = 0\n",
        "    train_l2_full = 0\n",
        "    for xx, yy in train_loader:\n",
        "        loss = 0\n",
        "        xx = xx.to(device)\n",
        "        yy = yy.to(device)\n",
        "\n",
        "        for t in range(0, T_out, step):\n",
        "            y = yy[..., t:t + step]\n",
        "            im = model(xx)\n",
        "            loss += myloss(im.reshape(batch_size, -1), y.reshape(batch_size, -1))\n",
        "\n",
        "            if t == 0:\n",
        "                pred = im\n",
        "            else:\n",
        "                pred = torch.cat((pred, im), -1)\n",
        "\n",
        "            xx = torch.cat((xx[..., step:], im), dim=-1)\n",
        "\n",
        "        train_l2_step += loss.item()\n",
        "        l2_full = myloss(pred.reshape(batch_size, -1), yy.reshape(batch_size, -1))\n",
        "        train_l2_full += l2_full.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    test_l2_step = 0\n",
        "    test_l2_full = 0\n",
        "    with torch.no_grad():\n",
        "        for xx, yy in test_loader:\n",
        "            loss = 0\n",
        "            xx = xx.to(device)\n",
        "            yy = yy.to(device)\n",
        "\n",
        "            for t in range(0, T_out, step):\n",
        "                y = yy[..., t:t + step]\n",
        "                im = model(xx)\n",
        "                loss += myloss(im.reshape(batch_size, -1), y.reshape(batch_size, -1))\n",
        "\n",
        "                if t == 0:\n",
        "                    pred = im\n",
        "                else:\n",
        "                    pred = torch.cat((pred, im), -1)\n",
        "\n",
        "                xx = torch.cat((xx[..., step:], im), dim=-1)\n",
        "\n",
        "            test_l2_step += loss.item()\n",
        "            test_l2_full += myloss(pred.reshape(batch_size, -1), yy.reshape(batch_size, -1)).item()\n",
        "\n",
        "    t2 = default_timer()\n",
        "    scheduler.step()\n",
        "    print(ep, t2 - t1, train_l2_step / ntrain / (T_out / step), train_l2_full / ntrain, test_l2_step / ntest / (T_out / step),\n",
        "          test_l2_full / ntest)\n",
        "    \n",
        "# add loss to name, with 4 decimals    \n",
        "final_training_loss = '{:.4f}'.format(test_l2_full / ntest)\n",
        "final_training_loss = final_training_loss.replace('.', '@')\n",
        "\n",
        "model_name = model_name+'_loss'+final_training_loss   \n",
        "model_full_path = model_path+model_name\n",
        "\n",
        "torch.save(model, model_full_path)\n",
        "\n",
        "#path_train_err = model_path+'results/'+model_name+'_train.txt'\n",
        "#path_test_err = model_path+'results/'+model_name+'_test.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1188897\n",
            "0 195.589008054 0.4491435212961833 0.4464748676300049 0.5622361839294434 0.5522071435451508\n",
            "1 196.14881256399997 0.3637638853200277 0.36046098203659055 0.47938893508911135 0.47184637236595156\n",
            "2 196.044176301 0.32253657030741373 0.31946302706400553 0.42001614322662356 0.4148394041061401\n",
            "3 196.05413192000003 0.2951982385253906 0.2916288748105367 0.38185322227478025 0.3760476493835449\n",
            "4 195.84856339099986 0.2830137729390462 0.27865598748524983 0.3793099124908447 0.37288418745994567\n",
            "5 195.81467456199994 0.2705756695556641 0.2657183201948802 0.3568531425476074 0.34952819848060607\n",
            "6 195.838900255 0.26355555814107257 0.25840408039093016 0.3329244386672974 0.32639084935188295\n",
            "7 195.82354034800005 0.25716421297709147 0.251802379989624 0.32894999694824223 0.3218329619169235\n",
            "8 195.75495048200014 0.25152340255737304 0.2460369982878367 0.3192900995254516 0.3122347991466522\n",
            "9 195.7804247879999 0.24853854148864746 0.2429251305739085 0.31559588699340824 0.308243950843811\n",
            "10 196.154117907 0.24432379501342774 0.23861536264419556 0.3030598979949951 0.2958247723579407\n",
            "11 196.2628921610003 0.24031498240152996 0.23452621733347576 0.29436333389282227 0.28712374579906463\n",
            "12 196.35869480099973 0.2360693925221761 0.23005020561218262 0.2852766324996948 0.2777582640647888\n",
            "13 196.0402447309998 0.23261244939168293 0.22645916601816812 0.28618179636001584 0.27834722101688386\n",
            "14 195.8866248710001 0.22924270095825197 0.22293859408696493 0.2795624966621399 0.27140850079059603\n",
            "15 195.9831579319998 0.22676891899108886 0.2203453193505605 0.2800548484802246 0.2720084738731384\n",
            "16 195.93460602699997 0.2246943465677897 0.21814176530838011 0.3001015281677246 0.29160851311683655\n",
            "17 195.92735398800005 0.22314625924428305 0.21656359235445657 0.2675136523246765 0.2592410633563995\n",
            "18 195.85495586100024 0.2201121011352539 0.21343966743151346 0.27383315753936766 0.2653769348859787\n",
            "19 195.79930883099996 0.2197428396987915 0.21307302544911702 0.2726109612464905 0.2643753007650375\n",
            "20 195.8477855469996 0.21743819314320884 0.21070747777620952 0.2780346164703369 0.2692501598596573\n",
            "21 195.84548478099987 0.2157408552424113 0.20897364554405212 0.26963988647460935 0.2612245670557022\n",
            "22 195.93310009800007 0.21412681126912436 0.207333620150884 0.27031865577697756 0.2617273235321045\n",
            "23 195.86818721100008 0.21160864475250243 0.20484190746943157 0.2610491229057312 0.2527181372642517\n",
            "24 195.86031219799952 0.2126956072107951 0.2058568413098653 0.25876157207489014 0.25020983231067656\n",
            "25 195.89439985799982 0.2102694589996338 0.20344140513737996 0.26117826490402224 0.2526767896413803\n",
            "26 195.77075672699993 0.20894232541402183 0.20216130555470785 0.2556286299705505 0.24736321115493776\n",
            "27 195.8104475709997 0.20717325422922772 0.20040456144014995 0.2585247303962707 0.24990444350242616\n",
            "28 195.9605535999999 0.2075864744313558 0.20077152463595072 0.2538208708763122 0.24543050479888917\n",
            "29 196.22969752000063 0.20728057325998944 0.20047233333587647 0.2703078184127808 0.2613686910867691\n",
            "30 196.25527974299985 0.20714295126597088 0.20033690137863158 0.2635857195854187 0.2550333799123764\n",
            "31 196.27037539699995 0.20683948043823244 0.19999174106915793 0.26985865936279296 0.2612167248725891\n",
            "32 196.1653292890005 0.20446575280507404 0.19766583805084229 0.2618213474273682 0.2531326730251312\n",
            "33 196.0529083500005 0.20459548137664796 0.1977750048796336 0.25609270372390747 0.24724045526981353\n",
            "34 196.23021619300016 0.2029190999476115 0.19613243198394775 0.2585703136444092 0.24985835289955138\n",
            "35 196.29025593500046 0.20312626670837403 0.1963179793516795 0.2515090863227844 0.24297912979125977\n",
            "36 196.1839448769997 0.20140873453776043 0.19463815038998922 0.25798364934921264 0.24907633399963378\n",
            "37 196.14773364899975 0.20253095682779948 0.19573420216242474 0.2516707701683044 0.2428489611148834\n",
            "38 195.98890641899925 0.2024116375096639 0.19558541027704876 0.24950266513824465 0.24104867100715638\n",
            "39 196.1342926590005 0.20111324502309164 0.1943269030412038 0.25811432886123653 0.24935761201381684\n",
            "40 196.1762615939997 0.19895727767944335 0.19221011877059938 0.2494899998664856 0.24095580077171325\n",
            "41 196.0034588010003 0.19950194133758545 0.1927541511853536 0.25293731975555417 0.24409615588188172\n",
            "42 196.08555610499934 0.20058167631785073 0.19376647952397663 0.2536554962158203 0.24500507354736328\n",
            "43 195.89927523799997 0.19809610500335692 0.19137040729522706 0.2423990537643433 0.2341190108060837\n",
            "44 195.99571972299964 0.198415384127299 0.19166683651606242 0.251634347820282 0.2432078205347061\n",
            "45 196.08800289099963 0.19827223044077555 0.19152252945899964 0.24740962982177733 0.23894746959209442\n",
            "46 196.07011913099996 0.19796144583384195 0.19122112731933594 0.2577506152153015 0.24904577946662904\n",
            "47 196.10697110699948 0.1977848069636027 0.1910516472975413 0.24207343654632568 0.23363782382011414\n",
            "48 195.97931501600033 0.19617318267822265 0.18948776165644327 0.24198174419403076 0.2337205159664154\n",
            "49 195.68868055799976 0.19692080198923748 0.19022359337806702 0.25052246856689453 0.24169345021247865\n",
            "50 195.65417393999996 0.19721989617665608 0.1905151197751363 0.24093909435272218 0.23258841347694398\n",
            "51 195.63957114599907 0.1964232884470622 0.1897147427558899 0.2488438346862793 0.24027870309352875\n",
            "52 195.6696817459997 0.19598252563476562 0.1892781364281972 0.2482923352241516 0.23956181633472443\n",
            "53 195.6177545849987 0.19583335145314534 0.18915097778638204 0.24198910760879516 0.233658682346344\n",
            "54 195.78853993899975 0.19490231835683186 0.18825167889595032 0.24526503171920777 0.2365817894935608\n",
            "55 195.75448724499984 0.1943336381530762 0.1876781421661377 0.2435291009902954 0.23500436222553253\n",
            "56 195.669192759 0.1947031243387858 0.18803538707097373 0.25055102548599245 0.2417471295595169\n",
            "57 195.72666003700033 0.19345937669118246 0.18683747771581014 0.24151631021499634 0.2331245048046112\n",
            "58 195.71321747399998 0.19419617350260415 0.18754974705378213 0.24346152896881104 0.23493099272251128\n",
            "59 195.68753168200055 0.19404203721364338 0.18739292904535929 0.2385090012550354 0.23029068160057067\n",
            "60 195.6795417430003 0.19242556911468506 0.1858325058142344 0.24044176511764528 0.23208078289031983\n",
            "61 195.70003625400022 0.19238213062286377 0.18575880551338195 0.24594038581848143 0.2376082673072815\n",
            "62 195.6878698519995 0.19252828182220458 0.18587151352564493 0.2359097412109375 0.22766611790657043\n",
            "63 195.65483136799958 0.19081826657613118 0.18404647099177043 0.245534344291687 0.23663433945178985\n",
            "64 195.68404603099953 0.18849961161295573 0.1816612521648407 0.23125177602767946 0.2228764023780823\n",
            "65 195.6760298040008 0.18491773916880289 0.17812024459838868 0.23500999698638916 0.2265204917192459\n",
            "66 195.66157471299994 0.1854519801457723 0.17863547916412353 0.22869302043914796 0.22026641297340394\n",
            "67 195.6867355659997 0.1824605182393392 0.17568854435284934 0.23008818225860597 0.22187994885444642\n",
            "68 195.7283914639993 0.18234712134043377 0.17552992550532023 0.23361323957443236 0.2250156091451645\n",
            "69 195.74184212099863 0.18040140059153237 0.1736024055639903 0.21953997421264648 0.21136278212070464\n",
            "70 195.69179619900024 0.18057601838429768 0.1737616191069285 0.21757129707336426 0.20936239993572234\n",
            "71 195.72367953699904 0.17920938593546548 0.17244882473945616 0.22345056438446048 0.21505103647708892\n",
            "72 195.75021419900077 0.17878759321848553 0.17200180207888285 0.22194461307525634 0.21373265862464905\n",
            "73 195.70253220300037 0.17750858399709066 0.1707845266977946 0.22428069591522215 0.21618681764602662\n",
            "74 195.74422417200003 0.17653120948791504 0.16981351037025452 0.2189981562614441 0.21066261959075927\n",
            "75 195.72906470499947 0.17757917064666748 0.17084466031392415 0.2170668683052063 0.20883928549289704\n",
            "76 195.7133465450006 0.17603495558420817 0.16933560557365418 0.2252451245307922 0.2169713340997696\n",
            "77 195.7564195089999 0.1751084200286865 0.16844514129956564 0.2219437264442444 0.2134575091600418\n",
            "78 195.8442817310006 0.17599754043579102 0.16929498987197875 0.2240754737854004 0.2158129951953888\n",
            "79 196.12308476699945 0.17683426860809326 0.17009044280052185 0.22207334661483763 0.21366686236858368\n",
            "80 196.1341651370003 0.174785555674235 0.16811948920885722 0.22080038948059083 0.21255298590660096\n",
            "81 195.82373218700013 0.17340744693756102 0.16679177355766298 0.2155166750907898 0.20721482574939729\n",
            "82 195.7815448220008 0.17316590834299722 0.1665468760331472 0.21817829484939577 0.20994730198383332\n",
            "83 195.7347178199998 0.17421043120066323 0.16753288445472717 0.22328332443237303 0.21508095812797545\n",
            "84 195.73060113999964 0.1741039827855428 0.16743009560902913 0.21639803876876834 0.2080996617078781\n",
            "85 195.6043503410001 0.17465806042989096 0.1679645473162333 0.2235185612678528 0.21529311883449553\n",
            "86 195.61406391399942 0.17422933349609376 0.16757247454325358 0.21693846397399902 0.20888243901729583\n",
            "87 195.57311569800004 0.1751336010869344 0.16846577165921528 0.22068796892166137 0.21241343820095063\n",
            "88 195.61209643799884 0.17381030484517415 0.16718998106320698 0.22421201162338256 0.21593607759475708\n",
            "89 195.58566396800234 0.17405958670298258 0.16744040654500325 0.21741955890655515 0.20898034644126892\n",
            "90 195.58268881099866 0.17410090681711832 0.16742669140497843 0.21498030490875247 0.20690381169319152\n",
            "91 195.55870173700168 0.17324348520914715 0.16660944677988687 0.21520664186477662 0.206851358294487\n",
            "92 195.56427090899888 0.17341124263763427 0.16681443282763164 0.21839753246307372 0.21013598120212554\n",
            "93 195.57564826400267 0.1724309988149007 0.1658265926837921 0.21431758279800417 0.20644094944000244\n",
            "94 195.5851436869998 0.17307438142140705 0.16647308502197267 0.2203259115219116 0.21215400099754333\n",
            "95 195.58218431999921 0.1737390149943034 0.16709655612309773 0.22045547151565553 0.2122107515335083\n",
            "96 195.55641668000317 0.17242809476216633 0.16584117225011188 0.21326395473480225 0.2052405903339386\n",
            "97 195.55536054299955 0.17060248881022136 0.16407548445065817 0.2108844434738159 0.2027203392982483\n",
            "98 195.58009879400197 0.17144891582489014 0.16488529539108276 0.22482752838134767 0.21661067938804626\n",
            "99 195.60364840400143 0.17080202388763427 0.16428897735277811 0.20757627182006835 0.1997456694841385\n",
            "100 195.68614752199937 0.15928048751831053 0.15329534584681193 0.19932101097106933 0.19169035744667054\n",
            "101 195.69435110300037 0.15826300477345784 0.15229317661921182 0.19980807285308838 0.19209399092197418\n",
            "102 195.7039581909994 0.15828559335072837 0.1523174852848053 0.2003931134223938 0.19260318100452423\n",
            "103 195.67584262699893 0.15798571838378905 0.1520010888258616 0.19537259473800658 0.18783256030082704\n",
            "104 196.0235298560001 0.157582064259847 0.15161897158622742 0.2003276309967041 0.19276310014724732\n",
            "105 195.95073893899826 0.15748018959045412 0.15151304335594176 0.1945102328300476 0.18697779321670532\n",
            "106 195.9257582350001 0.15739365871429442 0.15141415864626567 0.2007093761444092 0.19311700499057768\n",
            "107 195.93400808599836 0.1575947285970052 0.15161294782956442 0.1969736382484436 0.18951071488857268\n",
            "108 195.85855580600037 0.15737384506225588 0.15138831442197165 0.19382640600204468 0.18640953016281128\n",
            "109 195.8913152239984 0.15692961757659912 0.15094549810091654 0.19880340566635132 0.19106348943710327\n",
            "110 195.88419167299799 0.15686781330108643 0.15089468655586244 0.19595498638153078 0.18836652994155884\n",
            "111 195.70493252099914 0.15641823659261067 0.1504557117621104 0.19537378520965576 0.1878964273929596\n",
            "112 195.4818457879992 0.15577144723256428 0.14983325700759886 0.19164573955535888 0.18424664628505708\n",
            "113 195.4769765839992 0.1559187658691406 0.14995863455136618 0.19768487329483034 0.19006133270263673\n",
            "114 195.57599424499858 0.1559638949584961 0.14998602283795676 0.19557325162887573 0.1879815512895584\n",
            "115 195.62081587099965 0.1560749234263102 0.15009294401804607 0.19599977340698244 0.18854539740085602\n",
            "116 195.53275549200043 0.1549712815221151 0.14903241922060648 0.1899646185874939 0.1826158034801483\n",
            "117 195.56540603399844 0.15524028327941894 0.14928516613642376 0.1926827042579651 0.18530160474777221\n",
            "118 195.48861324800237 0.15437331459045409 0.148452184009552 0.19215423641204835 0.18468129193782806\n",
            "119 195.4428418380012 0.1545951204808553 0.14866133879025778 0.19275792760849 0.1852373615503311\n",
            "120 195.51387968700146 0.15384326493581135 0.14793604043324787 0.19036163301467895 0.18289956796169282\n",
            "121 195.52732770299917 0.15501087212880454 0.1490778156121572 0.1921318657875061 0.1846867311000824\n",
            "122 195.55681658399772 0.1540580354309082 0.14812325151761374 0.19125004606246948 0.1838276482820511\n",
            "123 195.4834562169999 0.15447365291595458 0.14852508994738262 0.18979918279647828 0.18228388500213624\n",
            "124 195.53367623899976 0.1535462174097697 0.14763696100711823 0.18923868522644044 0.1817388561964035\n",
            "125 195.5801605589986 0.15361602675120037 0.14768853216171265 0.1909858151435852 0.1834940983057022\n",
            "126 195.48415750500135 0.15318319169362388 0.14727851826349894 0.19116949577331543 0.18375798308849334\n",
            "127 195.6988655899986 0.1531407816696167 0.14722822394371032 0.1885256572723389 0.18120207571983338\n",
            "128 195.51602245600225 0.15268748653411865 0.14679969363212586 0.18822634420394896 0.1807770529985428\n",
            "129 195.50558332799847 0.15298542424519856 0.14708623956044514 0.19218650369644166 0.18465780973434448\n",
            "130 195.45249024499935 0.15206601212819418 0.14618575375874837 0.1887957402229309 0.18144947218894958\n",
            "131 195.4748734049972 0.15253834205627442 0.1466292727470398 0.19051812829971312 0.1831332424879074\n",
            "132 195.47210249899945 0.15188556405385337 0.14600046033859254 0.19093917875289917 0.1834009643793106\n",
            "133 195.45863387399731 0.1530688772201538 0.14712450550397238 0.19581557188034057 0.18850101208686829\n",
            "134 195.45448502200088 0.15221601278940838 0.1463190514564514 0.18879966793060304 0.18141740155220032\n",
            "135 195.4766182300009 0.15259504432678223 0.14667358992894491 0.1908408004760742 0.1834251298904419\n",
            "136 195.38747952700214 0.15234689798990886 0.1464331541220347 0.19114313468933106 0.18401404750347136\n",
            "137 195.387330982001 0.1515223158899943 0.1456332483291626 0.18891737546920778 0.18171160686016083\n",
            "138 195.40269104000254 0.15151663866678872 0.14562313787142436 0.18757158002853394 0.18021409213542938\n",
            "139 195.38020813699768 0.15074723823547362 0.1448807144800822 0.1945139307975769 0.1869422380924225\n",
            "140 195.35237517099813 0.1510270787302653 0.14514355985323588 0.18690990238189698 0.17961273980140685\n",
            "141 195.35086051100006 0.15211550952911376 0.14618464635213216 0.19275104150772096 0.18540935492515564\n",
            "142 195.36998898299862 0.15034228555043538 0.14450243590672812 0.188699808883667 0.18135830122232438\n",
            "143 195.32979149099992 0.1501653502782186 0.14432694110870362 0.18902634143829347 0.18175735819339753\n",
            "144 195.35479313299948 0.15058263092041016 0.1447008292833964 0.19169085578918457 0.18429750752449037\n",
            "145 195.4093065039997 0.15109592852274575 0.14519307022094727 0.1856130602836609 0.17837776517868043\n",
            "146 195.70042580399968 0.14967395969390868 0.1438270403703054 0.18806854448318483 0.18080816292762755\n",
            "147 195.8730973310012 0.1503985152943929 0.14451281282107034 0.188855495929718 0.18154559874534607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yphgOwKoKsHL"
      },
      "source": [
        "To avoid automatic runtime disconnection for inactivity [90 minutes]:\n",
        "\n",
        "_press ctlr+shift+i\n",
        "\n",
        "_then go to the console and type:\n",
        "\n",
        "```\n",
        "function ClickConnect(){\n",
        "  console.log(\"Connnect Clicked - Start\"); \n",
        "  document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "  console.log(\"Connnect Clicked - End\"); \n",
        "};\n",
        "setInterval(ClickConnect, 60000)\n",
        "```\n",
        "\n",
        "\n",
        "This js code will click the \"Connect\" button [top right of Colab notebook] every minute...\n",
        "Maximum runtime life remains 12 hours\n",
        "\n",
        "Found here: https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting?page=1&tab=votes#tab-top"
      ]
    }
  ]
}